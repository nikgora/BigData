# Завдання
1. [Опишіть парадигму Map-Reduce підходу ](#парадигма-map-reduce-підходу)
2. [Сформулюйте алгоритм множення матриць за допомогою map-reduce. Продемонструйте його роботу ](#алгоритм-множення-матриць-за-допомогою-map-reduce)
3. [Зменшіть розмірність датасету](#зменшення-розмірності-датасету-до-двох-за-допомогою-pca)$$ \begin {pmatrix}
  1 & 2 & 3 & 4 \\
  1 & 1 & 2 & 3 \\
  2 & 1 & 2  & 2
  \end{pmatrix}
$$ [до двох за допомогою PCA ](#зменшення-розмірності-датасету-до-двох-за-допомогою-pca)
4. [Проілюструйте роботу алгоритму k-means для двовимірного випадку ](#робота-
5. -k-means-для-двовимірного-вивадку)
5. [Доведіть, що існує оптимальна стратегія π∗, яка є стаціонарною (незалежною від часу) і детермінованою](#оптимальна-стратегія-π-*-яка-є-стаціонарною-незалежною-від-часу-і-детермінованою)
# Відповіді
1. # Парадигма Map-Reduce підходу
    - Map-Reduce - це підхід до обробки великих обсягів даних, який дозволяє розподілити обчислення на кілька вузлів кластера. 
    - Map-Reduce складається з двох основних етапів: Map і Reduce.
    - Map - це функція, яка приймає вхідні дані і перетворює їх на пари ключ-значення.
    - Reduce - це функція, яка приймає пари ключ-значення, збирає всі значення, що відповідають одному ключу, і обробляє їх.
    - Map-Reduce дозволяє обробляти великі обсяги даних шляхом розподілу обчислень на кілька вузлів кластера.
2. # Алгоритм множення матриць за допомогою map-reduce
    - Алгоритм множення матриць за допомогою Map-Reduce складається з двох етапів: Map і Reduce.
    - Map: для кожного елемента матриці A (i, j) та матриці B (j, k) створюється пара ключ-значення, де ключ - це індекс результуючого елемента матриці C (i, k), а значення - це відповідне добуток елементів матриць A та B.
    - Reduce: для кожного ключа (індекса результуючого елемента матриці C) обчислюється сума всіх значень, що відповідають цьому ключу, що і є значенням результуючого елемента матриці C.
    - Приклад роботи алгоритму множення матриць за допомогою Map-Reduce:
    ```
    A = [[1, 2],
         [3, 4]]
    B = [[5, 6],
         [7, 8]]
    Map:
    (0, 0) -> (0, 0, 1*5) -> (0, 0, 5)
    (0, 0) -> (0, 0, 2*7) -> (0, 0, 14)
    (0, 1) -> (0, 1, 1*6) -> (0, 1, 6)
    (0, 1) -> (0, 1, 2*8) -> (0, 1, 16)
    
    (1, 0) -> (1, 0, 3*5) -> (1, 0, 15)
    (1, 0) -> (1, 0, 4*7) -> (1, 0, 28)
    (1, 1) -> (1, 1, 3*6) -> (1, 1, 18)
    (1, 1) -> (1, 1, 4*8) -> (1, 1, 32)
    Reduce:
    (0, 0) -> 5 + 14 = 19
    (0, 1) -> 6 + 16 = 22
    (1, 0) -> 15 + 28 = 43
    (1, 1) -> 18 + 32 = 50
    Resultant Matrix C:
    [19, 22]
    [43, 50]
   ```
3. # Зменшення розмірності датасету до двох за допомогою PCA
    - PCA (Principal Component Analysis) - це метод зменшення розмірності даних, який дозволяє зменшити кількість змінних в датасеті, зберігаючи при цьому якнайбільше інформації.
    - PCA визначає головні компоненти даних, які мають найбільшу дисперсію, і проектує дані на ці компоненти.
    - Приклад зменшення розмірності датасету до двох за допомогою PCA:
    ### Початкова матриця:
    $$
    X = \begin{bmatrix}
    1 & 2 & 3 & 4 \\
    1 & 1 & 2 & 3 \\
    2 & 1 & 2 & 2
    \end{bmatrix}
    $$
    ---
    
    ### 1.1 **Обчислення середнього для кожного стовпця**
    Середнє по кожному стовпцю:
    $$
    \text{mean}[j] = \frac{\sum_{i=1}^n X[i,j]}{n}
    $$
    Результат:
    $$
    \text{mean} = [1.33, 1.33, 2.33, 3.00]
    $$
    ---
    
    ### 1.2 **Обчислення стандартного відхилення для кожного стовпця**
    Формула стандартного відхилення:
    $$
    \sigma[j] = \sqrt{\frac{\sum_{i=1}^n (X[i,j] - \text{mean}[j])^2}{n}}
    $$
    Результат:
    $$
    \sigma = [0.47, 0.47, 0.47, 0.82]
    $$
    
    ---
    
    ### 1.3 **Масштабування даних**
    Масштабовані дані:
    $$
    X_{\text{scaled}}[i,j] = \frac{X[i,j] - \text{mean}[j]}{\sigma[j]}
    $$
    
    Результат:
    $$
    X_{\text{scaled}} =
    \begin{bmatrix}
    -0.707 &  1.414 &  1.414 &  1.224 \\
    -0.707 & -0.707 & -0.707 &  0.000 \\
     1.414 & -0.707 & -0.707 & -1.225
    \end{bmatrix}
    $$
    
    ---
    
    ## 2. **Обчислення коваріаційної матриці**
    
    Коваріаційна матриця:
    $$
    \text{Cov}(X) = \frac{1}{n-1} X_{\text{scaled}}^T X_{\text{scaled}}
    $$
    
    ---
    
    ### 2.1 **Транспонування $$X_{\text{scaled}} $$:**
    $$
    X_{\text{scaled}}^T =
    \begin{bmatrix}
    -0.707 & -0.707 &  1.414 \\
     1.414 & -0.707 & -0.707 \\
     1.414 & -0.707 & -0.707 \\
     1.224 &  0.000 & -1.225
    \end{bmatrix}
    $$
    
    ---
    
    ### 2.2 **Множення $$ X_{\text{scaled}}^T \cdot X_{\text{scaled}} $$:**
    Обчислення кожного елемента. Наприклад:
    $$
    (1,1) = (-0.707)^2 + (-0.707)^2 + (1.414)^2 = 1.5
    $$
    
    Результат:
    $$
    \text{Covariance Matrix} =
    \begin{bmatrix}
     1.5 & -0.75 & -0.75 & -1.299 \\
    -0.75 &  1.5  &  1.5  &  1.299 \\
    -0.75 &  1.5  &  1.5  &  1.299 \\
    -1.299 & 1.299 & 1.299 &  1.5
    \end{bmatrix}
    $$
    
    ---
    
    ## 3. **Обчислення власних значень та векторів**
    
    Характеристичне рівняння:
    $$
    |\text{Cov} - \lambda I| = 0
    $$
    
    Результат власних значень:
    $$
    \lambda = [-6.54 \times 10^{-16}, -3.25 \times 10^{-16}, 1.016, 4.984]
    $$
    
    Власні вектори:
    $$
    \text{Eigenvectors} =
    \begin{bmatrix}
    -0.283 & -0.377 &  0.774 & -0.423 \\
    -0.423 &  0.613 &  0.425 &  0.514 \\
     0.707 & -0.237 &  0.425 &  0.514 \\
    -0.491 & -0.653 & -0.202 &  0.541
    \end{bmatrix}
    $$
    
    ---
    
    ## 4. **Сортування власних значень та векторів**
    
    Сортуємо власні значення у спадному порядку:
    $$
    \text{sorted_eigenvalues} = [4.984, 1.016, -3.25 \times 10^{-16}, -6.54 \times 10^{-16}]
    $$
    
    Відповідно, переставляємо вектори:
    $$
    \text{sorted_eigenvectors} =
    \begin{bmatrix}
    -0.423 &  0.774 \\
     0.514 &  0.425 \\
     0.514 &  0.425 \\
     0.541 & -0.202
    \end{bmatrix}
    $$
    
    ---
    
    ## 5. **Проекція на нові компоненти**
    
    ### 5.1 **Вибір $$ k = 2 $$ компонент**
    Проекційна матриця:
    $$
    \text{projection_matrix} =
    \begin{bmatrix}
    -0.423 &  0.774 \\
     0.514 &  0.425 \\
     0.514 &  0.425 \\
     0.541 & -0.202
    \end{bmatrix}
    $$
    
    ---
    
    ### 5.2 **Обчислення проекції**
    Проекція даних:
    $$
    X_{\text{reduced}} = X_{\text{scaled}} \cdot \text{projection_matrix}
    $$
    
    Результат:
    $$
    X_{\text{reduced}} =
    \begin{bmatrix}
     2.416 &  0.407 \\
    -0.428 & -1.148 \\
    -1.988 &  0.741
    \end{bmatrix}
    $$
    
    ---
   

 4. # Робота алгоритму k-means для двовимірного вивадку
    - Алгоритм k-means - це алгоритм кластеризації, який дозволяє розділити набір даних на кластери на основі їх відстані один від одного.
    - Алгоритм k-means працює наступним чином:
        1. Вибираємо кількість кластерів k.
        2. Випадковим чином ініціалізуємо центри кластерів.
        3. Для кожного елемента даних обчислюємо відстань до кожного центру кластера.
        4. Призначаємо кожен елемент до кластера з найближчим центром.
        5. Обчислюємо нові центри кластерів, як середнє значення всіх елементів у кожному кластері.
        6. Повторюємо кроки 3-5, поки центри кластерів не стабілізуються.
    - Приклад роботи алгоритму k-means для двовимірного випадку:
    ---
    
    ## Початкові дані
    
    1. **Вхідні точки:**
       $$
       \text{Data Points} = [(1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9), (10, 10)]
       $$
    
    2. **Ініціалізовані центри кластерів:**
       $$
       \text{Centroid}_1 = (2, 2), \quad \text{Centroid}_2 = (8, 8)
       $$
    
    ---
    
    ## Перша ітерація
    
    ### 1. Обчислення відстаней та призначення кластерів
    Відстані обчислені за формулою Евклідової відстані:
    $$
    \text{distance} = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}
    $$
    
    | Точка | Відстань до $$ (2, 2) $$ | Відстань до $$ (8, 8) $$ | Призначений кластер |
    |-------|---------------------------|---------------------------|----------------------|
    | (1, 1) | 1.41 | 9.90 | 1 |
    | (2, 2) | 0.00 | 8.49 | 1 |
    | (3, 3) | 1.41 | 7.07 | 1 |
    | (4, 4) | 2.83 | 5.66 | 1 |
    | (5, 5) | 4.24 | 4.24 | 1 |
    | (6, 6) | 5.66 | 2.83 | 2 |
    | (7, 7) | 7.07 | 1.41 | 2 |
    | (8, 8) | 8.49 | 0.00 | 2 |
    | (9, 9) | 9.90 | 1.41 | 2 |
    | (10, 10) | 11.31 | 2.83 | 2 |
    
    ---
    
    ### 2. Оновлення центрів кластерів
    Обчислюємо нові центри як середнє значення координат точок у кожному кластері:
    
    1. **Кластер 1** ($$ (1, 1), (2, 2), (3, 3), (4, 4), (5, 5) $$):
       $$
       \text{Centroid}_1 = \left( \frac{1+2+3+4+5}{5}, \frac{1+2+3+4+5}{5} \right) = (3, 3)
       $$
    
    2. **Кластер 2** ($$ (6, 6), (7, 7), (8, 8), (9, 9), (10, 10) $$):
       $$
       \text{Centroid}_2 = \left( \frac{6+7+8+9+10}{5}, \frac{6+7+8+9+10}{5} \right) = (8, 8)
       $$
    
    ---
    
    ## Друга ітерація
    
    ### 1. Обчислення відстаней з оновленими центроїдами
    | Точка | Відстань до $$ (3, 3) $$ | Відстань до $$ (8, 8) $$ | Призначений кластер |
    |-------|---------------------------|---------------------------|----------------------|
    | (1, 1) | 2.83 | 9.90 | 1 |
    | (2, 2) | 1.41 | 8.49 | 1 |
    | (3, 3) | 0.00 | 7.07 | 1 |
    | (4, 4) | 1.41 | 5.66 | 1 |
    | (5, 5) | 2.83 | 4.24 | 1 |
    | (6, 6) | 4.24 | 2.83 | 2 |
    | (7, 7) | 5.66 | 1.41 | 2 |
    | (8, 8) | 7.07 | 0.00 | 2 |
    | (9, 9) | 8.49 | 1.41 | 2 |
    | (10, 10) | 9.90 | 2.83 | 2 |
    
    ---
    
    ### 2. Оновлення центрів кластерів
    Обчислюємо нові центри кластерів. Оскільки призначення кластерів не змінилося, центри залишаються такими ж:
    
    1. **Кластер 1:**
       $$
       \text{Centroid}_1 = (3, 3)
       $$
    
    2. **Кластер 2:**
       $$
       \text{Centroid}_2 = (8, 8)
       $$
    
    ---
    
    ## Результат
    
    Після другої ітерації центри кластерів стабілізувалися, і алгоритм завершується.
    
    - **Фінальні центри кластерів:**
      $$
      \text{Centroid}_1 = (3, 3), \quad \text{Centroid}_2 = (8, 8)
      $$
    
      - **Розподіл точок по кластерах:**
        - Кластер 1: $$ (1, 1), (2, 2), (3, 3), (4, 4), (5, 5) $$
        - Кластер 2: $$ (6, 6), (7, 7), (8, 8), (9, 9), (10, 10) $$

5. # Оптимальна стратегія π∗, яка є стаціонарною (незалежною від часу) і детермінованою.
    - Оптимальна стратегія π∗ в контексті Марковського процесу прийняття рішень (MDP) є такою стратегією, яка максимізує очікувану суму винагород за всі можливі політики. Вона є стаціонарною, тобто не залежить від часу, і детермінованою, тобто для кожного стану визначає конкретну дію.  
   ## Доведення існування оптимальної стратегії
      - Визначення оптимальної стратегії π∗: Оптимальна стратегія π∗ визначається як така, що максимізує очікувану суму винагород: $$ V^*(s) = \max_{\pi} V^{\pi}(s) = \max_{\pi} \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, \pi(s_t)) \mid s_0 = s\right] $$ де $$ V^{\pi}(s) $$ - це функція вартості для стратегії π, яка представляє очікувану суму винагород, починаючи зі стану s і застосовуючи стратегію π.
      - Рівняння Беллмана для оптимальної стратегії: Оптимальна функція вартості V(s)  задовольняє рівняння Беллмана: $$ V(s)  = \max_{a \in A} \left[ R(s, a) + \gamma \sum_{s' \in S} P_{s,a}(s') V^*(s') \right] $$ де ( R(s, a) ) - це винагорода за дію a в стані s, ( P_{s,a}(s') ) - ймовірність переходу зі стану s в стан s' при дії a, а γ - коефіцієнт дисконтування.
      - Визначення оптимальної стратегії π∗: Оптимальна стратегія π∗ визначається як: $$ \pi^*(s) = \arg\max_{a \in A} \left[ R(s, a) + \gamma \sum_{s' \in S} P_{s,a}(s') V^*(s') \right] $$ Це означає, що для кожного стану s оптимальна стратегія π∗ вибирає дію a, яка максимізує очікувану суму винагород.
      - Теорема про існування оптимальної стратегії 
      - Теорема 7.1:
        - Існує оптимальна стратегія π∗, яка задовольняє умову: $$ \pi^* \geq \pi \quad \text{для всіх стратегій} \quad \pi $$ Це означає, що оптимальна стратегія π∗ є кращою або рівною будь-якій іншій стратегії π для всіх станів s ∈ S.
        - Всі оптимальні стратегії досягають однакової вартості: $$ V^{\pi}(s) = V(s) $$ Це означає, що всі оптимальні стратегії мають однакову функцію вартості $$ V^*(s) $$ для всіх станів s ∈ S.
        - Таким чином, оптимальна стратегія π∗ є стаціонарною (незалежною від часу) і детермінованою, і вона максимізує очікувану суму винагород для всіх можливих політик.
